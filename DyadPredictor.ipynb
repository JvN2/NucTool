{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Dyad Position Predictor\n",
                "Train a neural network to predict dyad positions on a per-position basis from encoded DNA sequences (0-7).\n",
                "\n",
                "## Data Format Expected:\n",
                "- Each sample: (dyad_positions, encoded_sequence)\n",
                "- dyad_positions: list of integers (positions where dyads occur)\n",
                "- encoded_sequence: list/array of integers (0-7, representing DNA bases)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from torch.utils.data import DataLoader, Dataset\n",
                "import matplotlib.pyplot as plt\n",
                "from tqdm import tqdm\n",
                "import json\n",
                "\n",
                "from ChromatinFibers import ChromatinFiber, simulate_chromatin_fibers"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Define the Model Architecture"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DyadPredictor(nn.Module):\n",
                "    \"\"\"Per-position dyad predictor using Conv1d and bidirectional context.\"\"\"\n",
                "    def __init__(self, vocab_size=8, embedding_dim=16, hidden_dim=64, num_layers=2, dropout=0.3):\n",
                "        super().__init__()\n",
                "        self.vocab_size = vocab_size\n",
                "        \n",
                "        # Embedding layer (map 0-7 to dense vectors)\n",
                "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
                "        \n",
                "        # Conv blocks for local context\n",
                "        self.conv1 = nn.Conv1d(embedding_dim, hidden_dim, kernel_size=5, padding=2)\n",
                "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=5, padding=2)\n",
                "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
                "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
                "        \n",
                "        # BiLSTM for global context\n",
                "        self.lstm = nn.LSTM(hidden_dim, hidden_dim // 2, num_layers=num_layers, \n",
                "                           bidirectional=True, dropout=dropout if num_layers > 1 else 0, \n",
                "                           batch_first=True)\n",
                "        \n",
                "        # Output head\n",
                "        self.fc = nn.Sequential(\n",
                "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
                "            nn.ReLU(),\n",
                "            nn.Dropout(dropout),\n",
                "            nn.Linear(hidden_dim // 2, 1)\n",
                "        )\n",
                "    \n",
                "    def forward(self, x):\n",
                "        \"\"\"Forward pass.\n",
                "        Args:\n",
                "            x: (batch_size, seq_len) - encoded sequence\n",
                "        Returns:\n",
                "            logits: (batch_size, seq_len, 1) - per-position dyad logits\n",
                "        \"\"\"\n",
                "        # Embedding: (batch, seq_len) -> (batch, seq_len, embed_dim)\n",
                "        x = self.embedding(x)\n",
                "        \n",
                "        # Conv blocks: (batch, seq_len, embed_dim) -> (batch, embed_dim, seq_len)\n",
                "        x = x.transpose(1, 2)\n",
                "        x = self.conv1(x)\n",
                "        x = self.bn1(x)\n",
                "        x = torch.relu(x)\n",
                "        \n",
                "        x = self.conv2(x)\n",
                "        x = self.bn2(x)\n",
                "        x = torch.relu(x)\n",
                "        \n",
                "        # Back to (batch, seq_len, hidden_dim)\n",
                "        x = x.transpose(1, 2)\n",
                "        \n",
                "        # LSTM: (batch, seq_len, hidden_dim) -> (batch, seq_len, hidden_dim)\n",
                "        x, _ = self.lstm(x)\n",
                "        \n",
                "        # Per-position classification: (batch, seq_len, hidden_dim) -> (batch, seq_len, 1)\n",
                "        logits = self.fc(x)\n",
                "        \n",
                "        return logits"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Define Custom Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DyadDataset(Dataset):\n",
                "    \"\"\"Dataset for dyad position prediction.\"\"\"\n",
                "    def __init__(self, data_list, max_seq_len=None):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            data_list: list of tuples (dyad_positions, encoded_sequence)\n",
                "                - dyad_positions: list of integers (positions with dyads)\n",
                "                - encoded_sequence: list/array of ints (0-7)\n",
                "            max_seq_len: optional, pad/truncate sequences to this length\n",
                "        \"\"\"\n",
                "        self.data = data_list\n",
                "        self.max_seq_len = max_seq_len or max(len(seq) for _, seq in data_list)\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.data)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        dyad_pos, encoded_seq = self.data[idx]\n",
                "        seq_len = len(encoded_seq)\n",
                "        \n",
                "        # Create binary label: 1 if dyad at position, 0 otherwise\n",
                "        label = np.zeros(seq_len, dtype=np.float32)\n",
                "        for pos in dyad_pos:\n",
                "            if 0 <= pos < seq_len:\n",
                "                label[pos] = 1.0\n",
                "        \n",
                "        # Convert to tensors\n",
                "        seq_tensor = torch.LongTensor(encoded_seq)\n",
                "        label_tensor = torch.FloatTensor(label)\n",
                "        \n",
                "        # Pad/truncate to max_seq_len\n",
                "        if seq_len < self.max_seq_len:\n",
                "            pad_len = self.max_seq_len - seq_len\n",
                "            seq_tensor = torch.nn.functional.pad(seq_tensor, (0, pad_len), value=0)\n",
                "            label_tensor = torch.nn.functional.pad(label_tensor, (0, pad_len), value=-1)  # -1 for padding\n",
                "        elif seq_len > self.max_seq_len:\n",
                "            seq_tensor = seq_tensor[:self.max_seq_len]\n",
                "            label_tensor = label_tensor[:self.max_seq_len]\n",
                "        \n",
                "        return seq_tensor, label_tensor"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Load Your Data\n",
                "Replace this with your actual data loading logic."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading data...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 100/100 [00:16<00:00,  6.05it/s]\n",
                        "100%|██████████| 100/100 [00:16<00:00,  6.05it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(array([ 73, 367, 550, 722, 871]), array([7, 7, 7, 2, 2, 3, 2, 0, 2, 2, 2, 1, 3, 1, 1, 2, 2, 3, 1, 1, 3, 1,\n",
                        "       1, 1, 3, 1, 0, 0, 3, 2, 2, 1, 3, 1, 3, 1, 1, 2, 1, 1, 2, 3, 0, 1,\n",
                        "       0, 2, 2, 3, 3, 1, 3, 0, 1, 1, 1, 2, 1, 3, 3, 0, 3, 3, 2, 1, 3, 1,\n",
                        "       1, 1, 2, 0, 0, 3, 1, 0, 0, 0, 2, 1, 3, 1, 0, 0, 2, 3, 0, 1, 0, 1,\n",
                        "       1, 2, 3, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 1, 3, 3, 0, 2, 1, 1, 1,\n",
                        "       2, 3, 2, 3, 0, 3, 3, 2, 2, 3, 0, 2, 0, 1, 3, 1, 0, 2, 3, 3, 2, 1,\n",
                        "       1, 2, 0, 0, 1, 0, 0, 0, 2, 3, 1, 2, 0, 7, 1, 4, 2, 2, 1, 7, 4, 1,\n",
                        "       3, 4, 1, 2, 4, 1, 1, 7, 2, 2, 1, 3, 1, 4, 2, 4, 2, 4, 2, 2, 2, 2,\n",
                        "       7, 1, 4, 4, 7, 4, 2, 2, 4, 2, 4, 1, 1, 4, 7, 1, 7, 2, 4, 7, 4, 1,\n",
                        "       2, 1, 7, 4, 7, 3, 4, 4, 7, 1, 7, 2, 7, 3, 1, 1, 2, 1, 2, 4, 7, 4,\n",
                        "       2, 3, 2, 1, 2, 2, 4, 2, 2, 2, 1, 4, 2, 2, 4, 2, 7, 7, 4, 1, 7, 4,\n",
                        "       1, 1, 4, 7, 4, 1, 1, 4, 7, 2, 7, 1, 4, 4, 1, 2, 4, 1, 7, 1, 0, 4,\n",
                        "       1, 7, 4, 4, 4, 1, 1, 1, 2, 2, 4, 4, 1, 2, 4, 7, 7, 2, 4, 7, 7, 7,\n",
                        "       1, 1, 1, 2, 4, 1, 1, 1, 1, 2, 1, 0, 3, 1, 2, 0, 0, 0, 0, 0, 2, 2,\n",
                        "       2, 1, 0, 0, 2, 1, 2, 3, 0, 1, 2, 2, 2, 1, 0, 0, 0, 0, 0, 1, 3, 0,\n",
                        "       1, 2, 2, 0, 3, 2, 2, 3, 0, 0, 0, 0, 2, 2, 3, 3, 0, 1, 0, 0, 0, 3,\n",
                        "       1, 2, 2, 0, 0, 2, 1, 1, 3, 1, 2, 2, 3, 2, 1, 3, 0, 2, 0, 1, 3, 1,\n",
                        "       0, 1, 3, 0, 0, 2, 0, 1, 3, 0, 3, 1, 2, 1, 1, 2, 0, 1, 0, 0, 1, 0,\n",
                        "       3, 0, 3, 0, 3, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 3, 1, 3, 0, 1, 3, 2,\n",
                        "       2, 2, 3, 3, 4, 3, 1, 3, 2, 2, 2, 2, 1, 2, 0, 2, 0, 1, 4, 1, 0, 3,\n",
                        "       1, 4, 1, 2, 7, 2, 1, 7, 1, 1, 0, 1, 3, 7, 4, 1, 4, 1, 2, 4, 1, 2,\n",
                        "       4, 1, 1, 7, 0, 3, 3, 2, 7, 2, 7, 7, 4, 7, 1, 1, 0, 0, 2, 1, 3, 1,\n",
                        "       0, 3, 3, 0, 2, 1, 0, 3, 1, 3, 0, 1, 3, 3, 1, 3, 1, 3, 2, 0, 2, 2,\n",
                        "       1, 0, 3, 0, 3, 2, 0, 1, 2, 2, 2, 0, 2, 3, 3, 3, 3, 0, 0, 3, 3, 1,\n",
                        "       1, 2, 1, 1, 2, 2, 0, 2, 1, 1, 0, 0, 2, 0, 3, 2, 2, 2, 0, 1, 3, 0,\n",
                        "       2, 0, 2, 2, 3, 2, 2, 3, 0, 2, 3, 2, 0, 0, 3, 1, 0, 0, 3, 1, 1, 1,\n",
                        "       3, 2, 1, 2, 2, 0, 3, 3, 0, 3, 1, 0, 3, 0, 2, 1, 0, 0, 1, 0, 3, 0,\n",
                        "       0, 0, 1, 0, 0, 0, 2, 3, 1, 3, 3, 3, 7, 1, 2, 2, 2, 3, 0, 3, 0, 0,\n",
                        "       0, 1, 3, 3, 0, 1, 1, 7, 1, 1, 1, 2, 4, 7, 7, 2, 0, 4, 1, 4, 2, 2,\n",
                        "       2, 7, 1, 2, 4, 2, 4, 2, 3, 0, 7, 3, 3, 7, 1, 1, 2, 3, 7, 1, 1, 1,\n",
                        "       3, 2, 2, 1, 3, 0, 2, 3, 2, 3, 3, 2, 3, 2, 2, 2, 2, 0, 2, 3, 1, 1,\n",
                        "       1, 1, 2, 0, 2, 3, 3, 1, 0, 3, 2, 1, 0, 0, 1, 3, 2, 1, 3, 1, 3, 0,\n",
                        "       0, 3, 3, 0, 3, 2, 3, 3, 1, 2, 1, 2, 1, 2, 0, 0, 1, 2, 3, 2, 3, 1,\n",
                        "       2, 2, 2, 2, 0, 1, 0, 2, 3, 2, 0, 0, 0, 0, 3, 2, 3, 2, 0, 3, 0, 3,\n",
                        "       0, 1, 2, 0, 2, 1, 3, 2, 3, 2, 0, 3, 1, 2, 2, 1, 1, 2, 3, 3, 1, 2,\n",
                        "       2, 1, 3, 3, 1, 3, 0, 2, 3, 1, 0, 2, 1, 3, 1, 0, 0, 2, 3, 0, 3, 1,\n",
                        "       1, 4, 0, 2, 1, 3, 1, 1, 0, 1, 0, 2, 0, 2, 1, 1, 0, 3, 1, 2, 1, 2,\n",
                        "       0, 0, 3, 2, 3, 0, 0, 3, 0, 3, 0, 3, 2, 3, 2, 0, 2, 1, 1, 2, 0, 2,\n",
                        "       0, 0, 1, 2, 2, 1, 3, 1, 0, 3, 3, 2, 2, 0, 2, 1, 0, 1, 2, 2, 2, 1,\n",
                        "       2, 1, 0, 0, 1, 3, 3, 0, 1, 0, 3, 2, 2, 1, 3, 3, 3, 1, 0, 1, 0, 2,\n",
                        "       2, 3, 1, 0, 3, 2, 2, 3, 1, 2, 2, 3, 2, 1, 2, 2, 2, 2, 1, 3, 2, 0,\n",
                        "       3, 2, 1, 0, 3, 1, 2, 0, 1, 0, 1, 2, 3, 2, 2, 2, 2, 1, 1, 0, 1, 3,\n",
                        "       1, 4, 0, 3, 3, 2, 4, 1, 1, 2, 2, 2, 7, 2, 7, 0, 3, 0, 1, 7, 2, 7,\n",
                        "       4, 2, 1, 4, 1, 4, 4, 7, 1, 7, 0, 1, 4, 4, 1, 2, 4, 2, 1, 2, 2, 2,\n",
                        "       1, 2, 2, 0, 2, 1, 7, 1, 1, 2, 1, 4, 1, 4, 4, 2, 1, 2, 4, 4, 7, 1,\n",
                        "       7, 1, 1, 1, 3, 4, 4, 2, 1, 4], dtype=int8))\n",
                        "Train: 70, Val: 15, Test: 15\n",
                        "Sample: dyad_positions=[ 73 367 550 722 871], seq_len=1000\n"
                    ]
                }
            ],
            "source": [
                "# EXAMPLE: Generate synthetic data (replace with your actual data)\n",
                "def generate_synthetic_data(n_samples=500, seq_len_min=50, seq_len_max=200):\n",
                "    \"\"\"Generate synthetic dyad/sequence pairs for demonstration.\"\"\"\n",
                "    dyad_positions, _, encoded_seq = simulate_chromatin_fibers(n_samples=100, length=1000)\n",
                "    data = [(dyads, seq) for dyads, seq in zip(dyad_positions, encoded_seq)]\n",
                "    \n",
                "    print(data[0])\n",
                "    return data\n",
                "\n",
                "# Generate or load your data here\n",
                "print(\"Loading data...\")\n",
                "all_data = generate_synthetic_data(n_samples=500)  # Replace with your data loading\n",
                "\n",
                "# Split into train/val/test\n",
                "n_train = int(0.7 * len(all_data))\n",
                "n_val = int(0.15 * len(all_data))\n",
                "\n",
                "train_data = all_data[:n_train]\n",
                "val_data = all_data[n_train:n_train+n_val]\n",
                "test_data = all_data[n_train+n_val:]\n",
                "\n",
                "print(f\"Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")\n",
                "print(f\"Sample: dyad_positions={train_data[0][0]}, seq_len={len(train_data[0][1])}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Create DataLoaders"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Max sequence length: 1000\n",
                        "DataLoaders created: train=3 batches, val=1, test=1\n"
                    ]
                }
            ],
            "source": [
                "# Create datasets and dataloaders\n",
                "train_dataset = DyadDataset(train_data)\n",
                "val_dataset = DyadDataset(val_data, max_seq_len=train_dataset.max_seq_len)\n",
                "test_dataset = DyadDataset(test_data, max_seq_len=train_dataset.max_seq_len)\n",
                "\n",
                "batch_size = 32\n",
                "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
                "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
                "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
                "\n",
                "print(f\"Max sequence length: {train_dataset.max_seq_len}\")\n",
                "print(f\"DataLoaders created: train={len(train_loader)} batches, val={len(val_loader)}, test={len(test_loader)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Initialize Model and Training Components"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: cpu\n",
                        "Model parameters: 78,401\n",
                        "Model parameters: 78,401\n"
                    ]
                }
            ],
            "source": [
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "model = DyadPredictor(vocab_size=8, embedding_dim=16, hidden_dim=64, num_layers=2, dropout=0.3)\n",
                "model = model.to(device)\n",
                "\n",
                "# Loss function: BCEWithLogitsLoss (combines sigmoid + BCE) with reduction='none' for masking\n",
                "criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
                "\n",
                "# Calculate positive class weight to handle class imbalance\n",
                "# Higher weight for positive (dyad) class since it's much rarer\n",
                "pos_weight = torch.tensor([10.0]).to(device)  # Weight positive class more heavily\n",
                "criterion_weighted = nn.BCEWithLogitsLoss(pos_weight=pos_weight, reduction='none')\n",
                "\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
                "# ReduceLROnPlateau without 'verbose' for compatibility with older PyTorch versions\n",
                "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
                "\n",
                "print(f\"Using device: {device}\")\n",
                "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
                "print(f\"Using weighted BCEWithLogitsLoss with pos_weight=10.0 to handle class imbalance\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting training...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Training: 100%|██████████| 3/3 [00:01<00:00,  2.23it/s]\n",
                        "Training: 100%|██████████| 3/3 [00:01<00:00,  2.23it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1/50 | Train Loss: 0.6306 | Val Loss: 0.6117\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Training: 100%|██████████| 3/3 [00:01<00:00,  2.29it/s]\n",
                        "Training: 100%|██████████| 3/3 [00:01<00:00,  2.29it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 27.33it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 27.33it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 2/50 | Train Loss: 0.5895 | Val Loss: 0.5815\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Training: 100%|██████████| 3/3 [00:01<00:00,  3.00it/s]\n",
                        "Training: 100%|██████████| 3/3 [00:01<00:00,  3.00it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 17.40it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 17.40it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 3/50 | Train Loss: 0.5472 | Val Loss: 0.5414\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Training: 100%|██████████| 3/3 [00:01<00:00,  2.75it/s]\n",
                        "Training: 100%|██████████| 3/3 [00:01<00:00,  2.75it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 12.92it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 12.92it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 4/50 | Train Loss: 0.4954 | Val Loss: 0.4808\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Training: 100%|██████████| 3/3 [00:01<00:00,  2.06it/s]\n",
                        "Training: 100%|██████████| 3/3 [00:01<00:00,  2.06it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 29.85it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 29.85it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 5/50 | Train Loss: 0.4281 | Val Loss: 0.3944\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Training: 100%|██████████| 3/3 [00:00<00:00,  4.29it/s]\n",
                        "Training: 100%|██████████| 3/3 [00:00<00:00,  4.29it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 28.85it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 28.85it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 6/50 | Train Loss: 0.3486 | Val Loss: 0.2967\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Training: 100%|██████████| 3/3 [00:00<00:00,  3.22it/s]\n",
                        "Training: 100%|██████████| 3/3 [00:00<00:00,  3.22it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 23.67it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 23.67it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 7/50 | Train Loss: 0.2678 | Val Loss: 0.2114\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Training: 100%|██████████| 3/3 [00:00<00:00,  3.95it/s]\n",
                        "Training: 100%|██████████| 3/3 [00:00<00:00,  3.95it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 27.17it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 27.17it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 8/50 | Train Loss: 0.1984 | Val Loss: 0.1462\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Training: 100%|██████████| 3/3 [00:00<00:00,  4.37it/s]\n",
                        "Training: 100%|██████████| 3/3 [00:00<00:00,  4.37it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 20.03it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 20.03it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 9/50 | Train Loss: 0.1436 | Val Loss: 0.1013\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Training: 100%|██████████| 3/3 [00:00<00:00,  4.27it/s]\n",
                        "Training: 100%|██████████| 3/3 [00:00<00:00,  4.27it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 18.42it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 18.42it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 10/50 | Train Loss: 0.1052 | Val Loss: 0.0723\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Training: 100%|██████████| 3/3 [00:00<00:00,  3.16it/s]\n",
                        "Training: 100%|██████████| 3/3 [00:00<00:00,  3.16it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 26.71it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 26.71it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 11/50 | Train Loss: 0.0804 | Val Loss: 0.0545\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Training: 100%|██████████| 3/3 [00:00<00:00,  4.27it/s]\n",
                        "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 27.91it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 27.91it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 12/50 | Train Loss: 0.0634 | Val Loss: 0.0438\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Training: 100%|██████████| 3/3 [00:00<00:00,  3.94it/s]\n",
                        "Training: 100%|██████████| 3/3 [00:00<00:00,  3.94it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 22.00it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 22.00it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 13/50 | Train Loss: 0.0535 | Val Loss: 0.0377\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Training: 100%|██████████| 3/3 [00:00<00:00,  3.36it/s]\n",
                        "Training: 100%|██████████| 3/3 [00:00<00:00,  3.36it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 22.83it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 22.83it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 14/50 | Train Loss: 0.0468 | Val Loss: 0.0344\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Training: 100%|██████████| 3/3 [00:00<00:00,  3.89it/s]\n",
                        "Training: 100%|██████████| 3/3 [00:00<00:00,  3.89it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 20.35it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 20.35it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 15/50 | Train Loss: 0.0430 | Val Loss: 0.0326\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Training: 100%|██████████| 3/3 [00:00<00:00,  3.62it/s]\n",
                        "Training: 100%|██████████| 3/3 [00:00<00:00,  3.62it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 27.71it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 27.71it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 16/50 | Train Loss: 0.0406 | Val Loss: 0.0317\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Training: 100%|██████████| 3/3 [00:00<00:00,  3.87it/s]\n",
                        "Training: 100%|██████████| 3/3 [00:00<00:00,  3.87it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 24.79it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 24.79it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 17/50 | Train Loss: 0.0382 | Val Loss: 0.0313\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Training: 100%|██████████| 3/3 [00:01<00:00,  2.90it/s]\n",
                        "Training: 100%|██████████| 3/3 [00:01<00:00,  2.90it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 23.83it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 23.83it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 18/50 | Train Loss: 0.0375 | Val Loss: 0.0312\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Training: 100%|██████████| 3/3 [00:00<00:00,  3.06it/s]\n",
                        "Training: 100%|██████████| 3/3 [00:00<00:00,  3.06it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 22.41it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 22.41it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 19/50 | Train Loss: 0.0383 | Val Loss: 0.0312\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Training: 100%|██████████| 3/3 [00:00<00:00,  3.32it/s]\n",
                        "Training: 100%|██████████| 3/3 [00:00<00:00,  3.32it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 17.82it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 17.82it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 20/50 | Train Loss: 0.0366 | Val Loss: 0.0313\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Training: 100%|██████████| 3/3 [00:00<00:00,  3.42it/s]\n",
                        "Training: 100%|██████████| 3/3 [00:00<00:00,  3.42it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 22.98it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 22.98it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 21/50 | Train Loss: 0.0370 | Val Loss: 0.0314\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Training: 100%|██████████| 3/3 [00:00<00:00,  3.87it/s]\n",
                        "Training: 100%|██████████| 3/3 [00:00<00:00,  3.87it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 23.41it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 23.41it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 22/50 | Train Loss: 0.0357 | Val Loss: 0.0315\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Training: 100%|██████████| 3/3 [00:00<00:00,  3.54it/s]\n",
                        "Training: 100%|██████████| 3/3 [00:00<00:00,  3.54it/s]\n",
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 26.25it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 23/50 | Train Loss: 0.0369 | Val Loss: 0.0316\n",
                        "Early stopping at epoch 23\n",
                        "Training completed!\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
                "    \"\"\"Train for one epoch.\"\"\"\n",
                "    model.train()\n",
                "    total_loss = 0\n",
                "    \n",
                "    for seq_batch, label_batch in tqdm(train_loader, desc=\"Training\"):\n",
                "        seq_batch = seq_batch.to(device)  # (batch, seq_len)\n",
                "        label_batch = label_batch.to(device)  # (batch, seq_len)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        \n",
                "        # Forward pass\n",
                "        logits = model(seq_batch)  # (batch, seq_len, 1)\n",
                "        logits = logits.squeeze(-1)  # (batch, seq_len)\n",
                "        \n",
                "        # Compute loss (ignore padding positions with label=-1)\n",
                "        loss_per_pos = criterion(logits, label_batch)  # (batch, seq_len)\n",
                "        mask = (label_batch >= 0).float()  # Mask out padding\n",
                "        loss = (loss_per_pos * mask).sum() / mask.sum().clamp(min=1)\n",
                "        \n",
                "        loss.backward()\n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
                "        optimizer.step()\n",
                "        \n",
                "        total_loss += loss.item()\n",
                "    \n",
                "    return total_loss / len(train_loader)\n",
                "\n",
                "def validate(model, val_loader, criterion, device):\n",
                "    \"\"\"Validate model.\"\"\"\n",
                "    model.eval()\n",
                "    total_loss = 0\n",
                "    all_preds = []\n",
                "    all_labels = []\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for seq_batch, label_batch in tqdm(val_loader, desc=\"Validating\"):\n",
                "            seq_batch = seq_batch.to(device)\n",
                "            label_batch = label_batch.to(device)\n",
                "            \n",
                "            logits = model(seq_batch)\n",
                "            logits = logits.squeeze(-1)\n",
                "            \n",
                "            loss_per_pos = criterion(logits, label_batch)\n",
                "            mask = (label_batch >= 0).float()\n",
                "            loss = (loss_per_pos * mask).sum() / mask.sum().clamp(min=1)\n",
                "            \n",
                "            total_loss += loss.item()\n",
                "            \n",
                "            # Store for metrics\n",
                "            probs = torch.sigmoid(logits)\n",
                "            all_preds.append(probs.cpu().numpy())\n",
                "            all_labels.append(label_batch.cpu().numpy())\n",
                "    \n",
                "    return total_loss / len(val_loader), all_preds, all_labels\n",
                "\n",
                "# Training loop\n",
                "epochs = 50\n",
                "train_losses = []\n",
                "val_losses = []\n",
                "best_val_loss = float('inf')\n",
                "patience = 5\n",
                "patience_counter = 0\n",
                "\n",
                "print(\"Starting training...\")\n",
                "for epoch in range(epochs):\n",
                "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
                "    val_loss, _, _ = validate(model, val_loader, criterion, device)\n",
                "    \n",
                "    train_losses.append(train_loss)\n",
                "    val_losses.append(val_loss)\n",
                "    \n",
                "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
                "    \n",
                "    # Early stopping\n",
                "    if val_loss < best_val_loss:\n",
                "        best_val_loss = val_loss\n",
                "        patience_counter = 0\n",
                "        # Save best model\n",
                "        torch.save(model.state_dict(), 'best_dyad_predictor.pt')\n",
                "    else:\n",
                "        patience_counter += 1\n",
                "        if patience_counter >= patience:\n",
                "            print(f\"Early stopping at epoch {epoch+1}\")\n",
                "            break\n",
                "    \n",
                "    scheduler.step(val_loss)\n",
                "\n",
                "print(\"Training completed!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Plot Training History"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 1000x500 with 1 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "plt.figure(figsize=(10, 5))\n",
                "plt.plot(train_losses, label='Train Loss', marker='o')\n",
                "plt.plot(val_losses, label='Val Loss', marker='s')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Loss')\n",
                "plt.title('Training History')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 8: Evaluate on Test Set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Validating: 100%|██████████| 1/1 [00:00<00:00, 27.47it/s]\n",
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Test Loss: 0.0333\n",
                        "\n",
                        "Test Metrics:\n",
                        "  Accuracy:  0.9947\n",
                        "  Precision: 0.0000\n",
                        "  Recall:    0.0000\n",
                        "  F1-Score:  0.0000\n",
                        "  ROC-AUC:   0.5087625670241287\n",
                        "\n",
                        "Test Metrics:\n",
                        "  Accuracy:  0.9947\n",
                        "  Precision: 0.0000\n",
                        "  Recall:    0.0000\n",
                        "  F1-Score:  0.0000\n",
                        "  ROC-AUC:   0.5087625670241287\n"
                    ]
                }
            ],
            "source": [
                "# Load best model\n",
                "model.load_state_dict(torch.load('best_dyad_predictor.pt'))\n",
                "\n",
                "# Evaluate on test set\n",
                "model.eval()\n",
                "test_loss, test_preds, test_labels = validate(model, test_loader, criterion, device)\n",
                "\n",
                "print(f\"Test Loss: {test_loss:.4f}\")\n",
                "\n",
                "# Compute metrics\n",
                "all_preds_flat = np.concatenate(test_preds).ravel()\n",
                "all_labels_flat = np.concatenate(test_labels).ravel()\n",
                "\n",
                "# Remove padding positions\n",
                "valid_mask = all_labels_flat >= 0\n",
                "all_preds_flat = all_preds_flat[valid_mask]\n",
                "all_labels_flat = all_labels_flat[valid_mask]\n",
                "\n",
                "# Threshold at 0.5\n",
                "predictions_binary = (all_preds_flat >= 0.5).astype(int)\n",
                "\n",
                "# Metrics: try sklearn, fallback to numpy implementations if missing\n",
                "try:\n",
                "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
                "    sklearn_available = True\n",
                "except Exception as e:\n",
                "    sklearn_available = False\n",
                "    import warnings\n",
                "    warnings.warn(\"scikit-learn not installed; using numpy fallback for basic metrics. Install with: pip install scikit-learn\")\n",
                "    def accuracy_score(y_true, y_pred):\n",
                "        y_true = np.asarray(y_true)\n",
                "        y_pred = np.asarray(y_pred)\n",
                "        return float((y_true == y_pred).mean())\n",
                "    def precision_score(y_true, y_pred, zero_division=0):\n",
                "        y_true = np.asarray(y_true)\n",
                "        y_pred = np.asarray(y_pred)\n",
                "        tp = int(((y_true == 1) & (y_pred == 1)).sum())\n",
                "        fp = int(((y_true == 0) & (y_pred == 1)).sum())\n",
                "        denom = tp + fp\n",
                "        if denom == 0:\n",
                "            return float(zero_division)\n",
                "        return float(tp / denom)\n",
                "    def recall_score(y_true, y_pred, zero_division=0):\n",
                "        y_true = np.asarray(y_true)\n",
                "        y_pred = np.asarray(y_pred)\n",
                "        tp = int(((y_true == 1) & (y_pred == 1)).sum())\n",
                "        fn = int(((y_true == 1) & (y_pred == 0)).sum())\n",
                "        denom = tp + fn\n",
                "        if denom == 0:\n",
                "            return float(zero_division)\n",
                "        return float(tp / denom)\n",
                "    def f1_score(y_true, y_pred, zero_division=0):\n",
                "        p = precision_score(y_true, y_pred, zero_division)\n",
                "        r = recall_score(y_true, y_pred, zero_division)\n",
                "        if (p + r) == 0:\n",
                "            return float(zero_division)\n",
                "        return 2 * (p * r) / (p + r)\n",
                "    def roc_auc_score(y_true, y_score):\n",
                "        y_true = np.asarray(y_true)\n",
                "        y_score = np.asarray(y_score)\n",
                "        # require both classes present\n",
                "        if len(np.unique(y_true)) < 2:\n",
                "            return float('nan')\n",
                "        # Sort scores descending\n",
                "        desc = np.argsort(-y_score)\n",
                "        y_true_sorted = y_true[desc]\n",
                "        # cumulative true/false positives\n",
                "        tp = np.cumsum(y_true_sorted == 1)\n",
                "        fp = np.cumsum(y_true_sorted == 0)\n",
                "        tp_total = tp[-1]\n",
                "        fp_total = fp[-1]\n",
                "        if tp_total == 0 or fp_total == 0:\n",
                "            return float('nan')\n",
                "        tpr = np.concatenate([[0.0], tp / tp_total])\n",
                "        fpr = np.concatenate([[0.0], fp / fp_total])\n",
                "        return float(np.trapz(tpr, fpr))\n",
                "\n",
                "# Calculate metrics\n",
                "acc = accuracy_score(all_labels_flat, predictions_binary)\n",
                "prec = precision_score(all_labels_flat, predictions_binary, zero_division=0)\n",
                "rec = recall_score(all_labels_flat, predictions_binary, zero_division=0)\n",
                "f1 = f1_score(all_labels_flat, predictions_binary, zero_division=0)\n",
                "auc = roc_auc_score(all_labels_flat, all_preds_flat)\n",
                "\n",
                "print(f\"\\nTest Metrics:\")\n",
                "print(f\"  Accuracy:  {acc:.4f}\")\n",
                "print(f\"  Precision: {prec:.4f}\")\n",
                "print(f\"  Recall:    {rec:.4f}\")\n",
                "print(f\"  F1-Score:  {f1:.4f}\")\n",
                "print(f\"  ROC-AUC:   {auc if not np.isnan(auc) else 'nan'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 9: Make Predictions on New Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Test sequence length: 1000\n",
                        "True dyad positions: [ 73 418 590 738 925]\n",
                        "Predicted dyad positions: []\n",
                        "\n",
                        "Per-position probabilities:\n",
                        "  Position 0: 0.022\n",
                        "  Position 1: 0.009\n",
                        "  Position 2: 0.006\n",
                        "  Position 3: 0.005\n",
                        "  Position 4: 0.005\n",
                        "  Position 5: 0.005\n",
                        "  Position 6: 0.005\n",
                        "  Position 7: 0.005\n",
                        "  Position 8: 0.005\n",
                        "  Position 9: 0.005\n",
                        "  Position 10: 0.005\n",
                        "  Position 11: 0.005\n",
                        "  Position 12: 0.005\n",
                        "  Position 13: 0.005\n",
                        "  Position 14: 0.005\n",
                        "  Position 15: 0.005\n",
                        "  Position 16: 0.005\n",
                        "  Position 17: 0.005\n",
                        "  Position 18: 0.005\n",
                        "  Position 19: 0.005\n",
                        "  Position 20: 0.005\n",
                        "  Position 21: 0.005\n",
                        "  Position 22: 0.005\n",
                        "  Position 23: 0.005\n",
                        "  Position 24: 0.005\n",
                        "  Position 25: 0.005\n",
                        "  Position 26: 0.005\n",
                        "  Position 27: 0.005\n",
                        "  Position 28: 0.005\n",
                        "  Position 29: 0.005\n",
                        "  Position 30: 0.005\n",
                        "  Position 31: 0.005\n",
                        "  Position 32: 0.005\n",
                        "  Position 33: 0.005\n",
                        "  Position 34: 0.005\n",
                        "  Position 35: 0.005\n",
                        "  Position 36: 0.005\n",
                        "  Position 37: 0.005\n",
                        "  Position 38: 0.005\n",
                        "  Position 39: 0.005\n",
                        "  Position 40: 0.005\n",
                        "  Position 41: 0.005\n",
                        "  Position 42: 0.005\n",
                        "  Position 43: 0.005\n",
                        "  Position 44: 0.005\n",
                        "  Position 45: 0.005\n",
                        "  Position 46: 0.005\n",
                        "  Position 47: 0.005\n",
                        "  Position 48: 0.005\n",
                        "  Position 49: 0.005\n",
                        "  Position 50: 0.005\n",
                        "  Position 51: 0.005\n",
                        "  Position 52: 0.005\n",
                        "  Position 53: 0.005\n",
                        "  Position 54: 0.005\n",
                        "  Position 55: 0.005\n",
                        "  Position 56: 0.005\n",
                        "  Position 57: 0.005\n",
                        "  Position 58: 0.005\n",
                        "  Position 59: 0.005\n",
                        "  Position 60: 0.005\n",
                        "  Position 61: 0.005\n",
                        "  Position 62: 0.005\n",
                        "  Position 63: 0.005\n",
                        "  Position 64: 0.005\n",
                        "  Position 65: 0.005\n",
                        "  Position 66: 0.005\n",
                        "  Position 67: 0.005\n",
                        "  Position 68: 0.005\n",
                        "  Position 69: 0.005\n",
                        "  Position 70: 0.005\n",
                        "  Position 71: 0.005\n",
                        "  Position 72: 0.005\n",
                        "  Position 73: 0.005\n",
                        "  Position 74: 0.005\n",
                        "  Position 75: 0.005\n",
                        "  Position 76: 0.005\n",
                        "  Position 77: 0.005\n",
                        "  Position 78: 0.005\n",
                        "  Position 79: 0.005\n",
                        "  Position 80: 0.005\n",
                        "  Position 81: 0.005\n",
                        "  Position 82: 0.005\n",
                        "  Position 83: 0.005\n",
                        "  Position 84: 0.005\n",
                        "  Position 85: 0.005\n",
                        "  Position 86: 0.005\n",
                        "  Position 87: 0.005\n",
                        "  Position 88: 0.005\n",
                        "  Position 89: 0.005\n",
                        "  Position 90: 0.005\n",
                        "  Position 91: 0.005\n",
                        "  Position 92: 0.005\n",
                        "  Position 93: 0.005\n",
                        "  Position 94: 0.005\n",
                        "  Position 95: 0.005\n",
                        "  Position 96: 0.005\n",
                        "  Position 97: 0.005\n",
                        "  Position 98: 0.005\n",
                        "  Position 99: 0.005\n",
                        "  Position 100: 0.005\n",
                        "  Position 101: 0.005\n",
                        "  Position 102: 0.005\n",
                        "  Position 103: 0.005\n",
                        "  Position 104: 0.005\n",
                        "  Position 105: 0.005\n",
                        "  Position 106: 0.005\n",
                        "  Position 107: 0.005\n",
                        "  Position 108: 0.005\n",
                        "  Position 109: 0.005\n",
                        "  Position 110: 0.005\n",
                        "  Position 111: 0.005\n",
                        "  Position 112: 0.005\n",
                        "  Position 113: 0.005\n",
                        "  Position 114: 0.005\n",
                        "  Position 115: 0.005\n",
                        "  Position 116: 0.005\n",
                        "  Position 117: 0.005\n",
                        "  Position 118: 0.005\n",
                        "  Position 119: 0.005\n",
                        "  Position 120: 0.005\n",
                        "  Position 121: 0.005\n",
                        "  Position 122: 0.005\n",
                        "  Position 123: 0.005\n",
                        "  Position 124: 0.005\n",
                        "  Position 125: 0.005\n",
                        "  Position 126: 0.005\n",
                        "  Position 127: 0.005\n",
                        "  Position 128: 0.005\n",
                        "  Position 129: 0.005\n",
                        "  Position 130: 0.005\n",
                        "  Position 131: 0.005\n",
                        "  Position 132: 0.005\n",
                        "  Position 133: 0.005\n",
                        "  Position 134: 0.005\n",
                        "  Position 135: 0.005\n",
                        "  Position 136: 0.005\n",
                        "  Position 137: 0.005\n",
                        "  Position 138: 0.005\n",
                        "  Position 139: 0.005\n",
                        "  Position 140: 0.005\n",
                        "  Position 141: 0.005\n",
                        "  Position 142: 0.005\n",
                        "  Position 143: 0.005\n",
                        "  Position 144: 0.005\n",
                        "  Position 145: 0.005\n",
                        "  Position 146: 0.005\n",
                        "  Position 147: 0.005\n",
                        "  Position 148: 0.005\n",
                        "  Position 149: 0.005\n",
                        "  Position 150: 0.005\n",
                        "  Position 151: 0.005\n",
                        "  Position 152: 0.005\n",
                        "  Position 153: 0.005\n",
                        "  Position 154: 0.005\n",
                        "  Position 155: 0.005\n",
                        "  Position 156: 0.005\n",
                        "  Position 157: 0.005\n",
                        "  Position 158: 0.005\n",
                        "  Position 159: 0.005\n",
                        "  Position 160: 0.005\n",
                        "  Position 161: 0.005\n",
                        "  Position 162: 0.005\n",
                        "  Position 163: 0.005\n",
                        "  Position 164: 0.005\n",
                        "  Position 165: 0.005\n",
                        "  Position 166: 0.005\n",
                        "  Position 167: 0.005\n",
                        "  Position 168: 0.005\n",
                        "  Position 169: 0.005\n",
                        "  Position 170: 0.005\n",
                        "  Position 171: 0.005\n",
                        "  Position 172: 0.005\n",
                        "  Position 173: 0.005\n",
                        "  Position 174: 0.005\n",
                        "  Position 175: 0.005\n",
                        "  Position 176: 0.005\n",
                        "  Position 177: 0.005\n",
                        "  Position 178: 0.005\n",
                        "  Position 179: 0.005\n",
                        "  Position 180: 0.005\n",
                        "  Position 181: 0.005\n",
                        "  Position 182: 0.005\n",
                        "  Position 183: 0.005\n",
                        "  Position 184: 0.005\n",
                        "  Position 185: 0.005\n",
                        "  Position 186: 0.005\n",
                        "  Position 187: 0.005\n",
                        "  Position 188: 0.005\n",
                        "  Position 189: 0.005\n",
                        "  Position 190: 0.005\n",
                        "  Position 191: 0.005\n",
                        "  Position 192: 0.005\n",
                        "  Position 193: 0.005\n",
                        "  Position 194: 0.005\n",
                        "  Position 195: 0.005\n",
                        "  Position 196: 0.005\n",
                        "  Position 197: 0.005\n",
                        "  Position 198: 0.005\n",
                        "  Position 199: 0.005\n",
                        "  Position 200: 0.005\n",
                        "  Position 201: 0.005\n",
                        "  Position 202: 0.005\n",
                        "  Position 203: 0.005\n",
                        "  Position 204: 0.005\n",
                        "  Position 205: 0.005\n",
                        "  Position 206: 0.005\n",
                        "  Position 207: 0.005\n",
                        "  Position 208: 0.005\n",
                        "  Position 209: 0.005\n",
                        "  Position 210: 0.005\n",
                        "  Position 211: 0.005\n",
                        "  Position 212: 0.005\n",
                        "  Position 213: 0.005\n",
                        "  Position 214: 0.005\n",
                        "  Position 215: 0.005\n",
                        "  Position 216: 0.005\n",
                        "  Position 217: 0.005\n",
                        "  Position 218: 0.005\n",
                        "  Position 219: 0.005\n",
                        "  Position 220: 0.005\n",
                        "  Position 221: 0.005\n",
                        "  Position 222: 0.005\n",
                        "  Position 223: 0.005\n",
                        "  Position 224: 0.005\n",
                        "  Position 225: 0.005\n",
                        "  Position 226: 0.005\n",
                        "  Position 227: 0.005\n",
                        "  Position 228: 0.005\n",
                        "  Position 229: 0.005\n",
                        "  Position 230: 0.005\n",
                        "  Position 231: 0.005\n",
                        "  Position 232: 0.005\n",
                        "  Position 233: 0.005\n",
                        "  Position 234: 0.005\n",
                        "  Position 235: 0.005\n",
                        "  Position 236: 0.005\n",
                        "  Position 237: 0.005\n",
                        "  Position 238: 0.005\n",
                        "  Position 239: 0.005\n",
                        "  Position 240: 0.005\n",
                        "  Position 241: 0.005\n",
                        "  Position 242: 0.005\n",
                        "  Position 243: 0.005\n",
                        "  Position 244: 0.005\n",
                        "  Position 245: 0.005\n",
                        "  Position 246: 0.005\n",
                        "  Position 247: 0.005\n",
                        "  Position 248: 0.005\n",
                        "  Position 249: 0.005\n",
                        "  Position 250: 0.005\n",
                        "  Position 251: 0.005\n",
                        "  Position 252: 0.005\n",
                        "  Position 253: 0.005\n",
                        "  Position 254: 0.005\n",
                        "  Position 255: 0.005\n",
                        "  Position 256: 0.005\n",
                        "  Position 257: 0.005\n",
                        "  Position 258: 0.005\n",
                        "  Position 259: 0.005\n",
                        "  Position 260: 0.005\n",
                        "  Position 261: 0.005\n",
                        "  Position 262: 0.005\n",
                        "  Position 263: 0.005\n",
                        "  Position 264: 0.005\n",
                        "  Position 265: 0.005\n",
                        "  Position 266: 0.005\n",
                        "  Position 267: 0.005\n",
                        "  Position 268: 0.005\n",
                        "  Position 269: 0.005\n",
                        "  Position 270: 0.005\n",
                        "  Position 271: 0.005\n",
                        "  Position 272: 0.005\n",
                        "  Position 273: 0.005\n",
                        "  Position 274: 0.005\n",
                        "  Position 275: 0.005\n",
                        "  Position 276: 0.005\n",
                        "  Position 277: 0.005\n",
                        "  Position 278: 0.005\n",
                        "  Position 279: 0.005\n",
                        "  Position 280: 0.005\n",
                        "  Position 281: 0.005\n",
                        "  Position 282: 0.005\n",
                        "  Position 283: 0.005\n",
                        "  Position 284: 0.005\n",
                        "  Position 285: 0.005\n",
                        "  Position 286: 0.005\n",
                        "  Position 287: 0.005\n",
                        "  Position 288: 0.005\n",
                        "  Position 289: 0.005\n",
                        "  Position 290: 0.005\n",
                        "  Position 291: 0.005\n",
                        "  Position 292: 0.005\n",
                        "  Position 293: 0.005\n",
                        "  Position 294: 0.005\n",
                        "  Position 295: 0.005\n",
                        "  Position 296: 0.005\n",
                        "  Position 297: 0.005\n",
                        "  Position 298: 0.005\n",
                        "  Position 299: 0.005\n",
                        "  Position 300: 0.005\n",
                        "  Position 301: 0.005\n",
                        "  Position 302: 0.005\n",
                        "  Position 303: 0.005\n",
                        "  Position 304: 0.005\n",
                        "  Position 305: 0.005\n",
                        "  Position 306: 0.005\n",
                        "  Position 307: 0.005\n",
                        "  Position 308: 0.005\n",
                        "  Position 309: 0.005\n",
                        "  Position 310: 0.005\n",
                        "  Position 311: 0.005\n",
                        "  Position 312: 0.005\n",
                        "  Position 313: 0.005\n",
                        "  Position 314: 0.005\n",
                        "  Position 315: 0.005\n",
                        "  Position 316: 0.005\n",
                        "  Position 317: 0.005\n",
                        "  Position 318: 0.005\n",
                        "  Position 319: 0.005\n",
                        "  Position 320: 0.005\n",
                        "  Position 321: 0.005\n",
                        "  Position 322: 0.005\n",
                        "  Position 323: 0.005\n",
                        "  Position 324: 0.005\n",
                        "  Position 325: 0.005\n",
                        "  Position 326: 0.005\n",
                        "  Position 327: 0.005\n",
                        "  Position 328: 0.005\n",
                        "  Position 329: 0.005\n",
                        "  Position 330: 0.005\n",
                        "  Position 331: 0.005\n",
                        "  Position 332: 0.005\n",
                        "  Position 333: 0.005\n",
                        "  Position 334: 0.005\n",
                        "  Position 335: 0.005\n",
                        "  Position 336: 0.005\n",
                        "  Position 337: 0.005\n",
                        "  Position 338: 0.005\n",
                        "  Position 339: 0.005\n",
                        "  Position 340: 0.005\n",
                        "  Position 341: 0.005\n",
                        "  Position 342: 0.005\n",
                        "  Position 343: 0.005\n",
                        "  Position 344: 0.005\n",
                        "  Position 345: 0.005\n",
                        "  Position 346: 0.005\n",
                        "  Position 347: 0.005\n",
                        "  Position 348: 0.005\n",
                        "  Position 349: 0.005\n",
                        "  Position 350: 0.005\n",
                        "  Position 351: 0.005\n",
                        "  Position 352: 0.005\n",
                        "  Position 353: 0.005\n",
                        "  Position 354: 0.005\n",
                        "  Position 355: 0.005\n",
                        "  Position 356: 0.005\n",
                        "  Position 357: 0.005\n",
                        "  Position 358: 0.005\n",
                        "  Position 359: 0.005\n",
                        "  Position 360: 0.005\n",
                        "  Position 361: 0.005\n",
                        "  Position 362: 0.005\n",
                        "  Position 363: 0.005\n",
                        "  Position 364: 0.005\n",
                        "  Position 365: 0.005\n",
                        "  Position 366: 0.005\n",
                        "  Position 367: 0.005\n",
                        "  Position 368: 0.005\n",
                        "  Position 369: 0.005\n",
                        "  Position 370: 0.005\n",
                        "  Position 371: 0.005\n",
                        "  Position 372: 0.005\n",
                        "  Position 373: 0.005\n",
                        "  Position 374: 0.005\n",
                        "  Position 375: 0.005\n",
                        "  Position 376: 0.005\n",
                        "  Position 377: 0.005\n",
                        "  Position 378: 0.005\n",
                        "  Position 379: 0.005\n",
                        "  Position 380: 0.005\n",
                        "  Position 381: 0.005\n",
                        "  Position 382: 0.005\n",
                        "  Position 383: 0.005\n",
                        "  Position 384: 0.005\n",
                        "  Position 385: 0.005\n",
                        "  Position 386: 0.005\n",
                        "  Position 387: 0.005\n",
                        "  Position 388: 0.005\n",
                        "  Position 389: 0.005\n",
                        "  Position 390: 0.005\n",
                        "  Position 391: 0.005\n",
                        "  Position 392: 0.005\n",
                        "  Position 393: 0.005\n",
                        "  Position 394: 0.005\n",
                        "  Position 395: 0.005\n",
                        "  Position 396: 0.005\n",
                        "  Position 397: 0.005\n",
                        "  Position 398: 0.005\n",
                        "  Position 399: 0.005\n",
                        "  Position 400: 0.005\n",
                        "  Position 401: 0.005\n",
                        "  Position 402: 0.005\n",
                        "  Position 403: 0.005\n",
                        "  Position 404: 0.005\n",
                        "  Position 405: 0.005\n",
                        "  Position 406: 0.005\n",
                        "  Position 407: 0.005\n",
                        "  Position 408: 0.005\n",
                        "  Position 409: 0.005\n",
                        "  Position 410: 0.005\n",
                        "  Position 411: 0.005\n",
                        "  Position 412: 0.005\n",
                        "  Position 413: 0.005\n",
                        "  Position 414: 0.005\n",
                        "  Position 415: 0.005\n",
                        "  Position 416: 0.005\n",
                        "  Position 417: 0.005\n",
                        "  Position 418: 0.005\n",
                        "  Position 419: 0.005\n",
                        "  Position 420: 0.005\n",
                        "  Position 421: 0.005\n",
                        "  Position 422: 0.005\n",
                        "  Position 423: 0.005\n",
                        "  Position 424: 0.005\n",
                        "  Position 425: 0.005\n",
                        "  Position 426: 0.005\n",
                        "  Position 427: 0.005\n",
                        "  Position 428: 0.005\n",
                        "  Position 429: 0.005\n",
                        "  Position 430: 0.005\n",
                        "  Position 431: 0.005\n",
                        "  Position 432: 0.005\n",
                        "  Position 433: 0.005\n",
                        "  Position 434: 0.005\n",
                        "  Position 435: 0.005\n",
                        "  Position 436: 0.005\n",
                        "  Position 437: 0.005\n",
                        "  Position 438: 0.005\n",
                        "  Position 439: 0.005\n",
                        "  Position 440: 0.005\n",
                        "  Position 441: 0.005\n",
                        "  Position 442: 0.005\n",
                        "  Position 443: 0.005\n",
                        "  Position 444: 0.005\n",
                        "  Position 445: 0.005\n",
                        "  Position 446: 0.005\n",
                        "  Position 447: 0.005\n",
                        "  Position 448: 0.005\n",
                        "  Position 449: 0.005\n",
                        "  Position 450: 0.005\n",
                        "  Position 451: 0.005\n",
                        "  Position 452: 0.005\n",
                        "  Position 453: 0.005\n",
                        "  Position 454: 0.005\n",
                        "  Position 455: 0.005\n",
                        "  Position 456: 0.005\n",
                        "  Position 457: 0.005\n",
                        "  Position 458: 0.005\n",
                        "  Position 459: 0.005\n",
                        "  Position 460: 0.005\n",
                        "  Position 461: 0.005\n",
                        "  Position 462: 0.005\n",
                        "  Position 463: 0.005\n",
                        "  Position 464: 0.005\n",
                        "  Position 465: 0.005\n",
                        "  Position 466: 0.005\n",
                        "  Position 467: 0.005\n",
                        "  Position 468: 0.005\n",
                        "  Position 469: 0.005\n",
                        "  Position 470: 0.005\n",
                        "  Position 471: 0.005\n",
                        "  Position 472: 0.005\n",
                        "  Position 473: 0.005\n",
                        "  Position 474: 0.005\n",
                        "  Position 475: 0.005\n",
                        "  Position 476: 0.005\n",
                        "  Position 477: 0.005\n",
                        "  Position 478: 0.005\n",
                        "  Position 479: 0.005\n",
                        "  Position 480: 0.005\n",
                        "  Position 481: 0.005\n",
                        "  Position 482: 0.005\n",
                        "  Position 483: 0.005\n",
                        "  Position 484: 0.005\n",
                        "  Position 485: 0.005\n",
                        "  Position 486: 0.005\n",
                        "  Position 487: 0.005\n",
                        "  Position 488: 0.005\n",
                        "  Position 489: 0.005\n",
                        "  Position 490: 0.005\n",
                        "  Position 491: 0.005\n",
                        "  Position 492: 0.005\n",
                        "  Position 493: 0.005\n",
                        "  Position 494: 0.005\n",
                        "  Position 495: 0.005\n",
                        "  Position 496: 0.005\n",
                        "  Position 497: 0.005\n",
                        "  Position 498: 0.005\n",
                        "  Position 499: 0.005\n",
                        "  Position 500: 0.005\n",
                        "  Position 501: 0.005\n",
                        "  Position 502: 0.005\n",
                        "  Position 503: 0.005\n",
                        "  Position 504: 0.005\n",
                        "  Position 505: 0.005\n",
                        "  Position 506: 0.005\n",
                        "  Position 507: 0.005\n",
                        "  Position 508: 0.005\n",
                        "  Position 509: 0.005\n",
                        "  Position 510: 0.005\n",
                        "  Position 511: 0.005\n",
                        "  Position 512: 0.005\n",
                        "  Position 513: 0.005\n",
                        "  Position 514: 0.005\n",
                        "  Position 515: 0.005\n",
                        "  Position 516: 0.005\n",
                        "  Position 517: 0.005\n",
                        "  Position 518: 0.005\n",
                        "  Position 519: 0.005\n",
                        "  Position 520: 0.005\n",
                        "  Position 521: 0.005\n",
                        "  Position 522: 0.005\n",
                        "  Position 523: 0.005\n",
                        "  Position 524: 0.005\n",
                        "  Position 525: 0.005\n",
                        "  Position 526: 0.005\n",
                        "  Position 527: 0.005\n",
                        "  Position 528: 0.005\n",
                        "  Position 529: 0.005\n",
                        "  Position 530: 0.005\n",
                        "  Position 531: 0.005\n",
                        "  Position 532: 0.005\n",
                        "  Position 533: 0.005\n",
                        "  Position 534: 0.005\n",
                        "  Position 535: 0.005\n",
                        "  Position 536: 0.005\n",
                        "  Position 537: 0.005\n",
                        "  Position 538: 0.005\n",
                        "  Position 539: 0.005\n",
                        "  Position 540: 0.005\n",
                        "  Position 541: 0.005\n",
                        "  Position 542: 0.005\n",
                        "  Position 543: 0.005\n",
                        "  Position 544: 0.005\n",
                        "  Position 545: 0.005\n",
                        "  Position 546: 0.005\n",
                        "  Position 547: 0.005\n",
                        "  Position 548: 0.005\n",
                        "  Position 549: 0.005\n",
                        "  Position 550: 0.005\n",
                        "  Position 551: 0.005\n",
                        "  Position 552: 0.005\n",
                        "  Position 553: 0.005\n",
                        "  Position 554: 0.005\n",
                        "  Position 555: 0.005\n",
                        "  Position 556: 0.005\n",
                        "  Position 557: 0.005\n",
                        "  Position 558: 0.005\n",
                        "  Position 559: 0.005\n",
                        "  Position 560: 0.005\n",
                        "  Position 561: 0.005\n",
                        "  Position 562: 0.005\n",
                        "  Position 563: 0.005\n",
                        "  Position 564: 0.005\n",
                        "  Position 565: 0.005\n",
                        "  Position 566: 0.005\n",
                        "  Position 567: 0.005\n",
                        "  Position 568: 0.005\n",
                        "  Position 569: 0.005\n",
                        "  Position 570: 0.005\n",
                        "  Position 571: 0.005\n",
                        "  Position 572: 0.005\n",
                        "  Position 573: 0.005\n",
                        "  Position 574: 0.005\n",
                        "  Position 575: 0.005\n",
                        "  Position 576: 0.005\n",
                        "  Position 577: 0.005\n",
                        "  Position 578: 0.005\n",
                        "  Position 579: 0.005\n",
                        "  Position 580: 0.005\n",
                        "  Position 581: 0.005\n",
                        "  Position 582: 0.005\n",
                        "  Position 583: 0.005\n",
                        "  Position 584: 0.005\n",
                        "  Position 585: 0.005\n",
                        "  Position 586: 0.005\n",
                        "  Position 587: 0.005\n",
                        "  Position 588: 0.005\n",
                        "  Position 589: 0.005\n",
                        "  Position 590: 0.005\n",
                        "  Position 591: 0.005\n",
                        "  Position 592: 0.005\n",
                        "  Position 593: 0.005\n",
                        "  Position 594: 0.005\n",
                        "  Position 595: 0.005\n",
                        "  Position 596: 0.005\n",
                        "  Position 597: 0.005\n",
                        "  Position 598: 0.005\n",
                        "  Position 599: 0.005\n",
                        "  Position 600: 0.005\n",
                        "  Position 601: 0.005\n",
                        "  Position 602: 0.005\n",
                        "  Position 603: 0.005\n",
                        "  Position 604: 0.005\n",
                        "  Position 605: 0.005\n",
                        "  Position 606: 0.005\n",
                        "  Position 607: 0.005\n",
                        "  Position 608: 0.005\n",
                        "  Position 609: 0.005\n",
                        "  Position 610: 0.005\n",
                        "  Position 611: 0.005\n",
                        "  Position 612: 0.005\n",
                        "  Position 613: 0.005\n",
                        "  Position 614: 0.005\n",
                        "  Position 615: 0.005\n",
                        "  Position 616: 0.005\n",
                        "  Position 617: 0.005\n",
                        "  Position 618: 0.005\n",
                        "  Position 619: 0.005\n",
                        "  Position 620: 0.005\n",
                        "  Position 621: 0.005\n",
                        "  Position 622: 0.005\n",
                        "  Position 623: 0.005\n",
                        "  Position 624: 0.005\n",
                        "  Position 625: 0.005\n",
                        "  Position 626: 0.005\n",
                        "  Position 627: 0.005\n",
                        "  Position 628: 0.005\n",
                        "  Position 629: 0.005\n",
                        "  Position 630: 0.005\n",
                        "  Position 631: 0.005\n",
                        "  Position 632: 0.005\n",
                        "  Position 633: 0.005\n",
                        "  Position 634: 0.005\n",
                        "  Position 635: 0.005\n",
                        "  Position 636: 0.005\n",
                        "  Position 637: 0.005\n",
                        "  Position 638: 0.005\n",
                        "  Position 639: 0.005\n",
                        "  Position 640: 0.005\n",
                        "  Position 641: 0.005\n",
                        "  Position 642: 0.005\n",
                        "  Position 643: 0.005\n",
                        "  Position 644: 0.005\n",
                        "  Position 645: 0.005\n",
                        "  Position 646: 0.005\n",
                        "  Position 647: 0.005\n",
                        "  Position 648: 0.005\n",
                        "  Position 649: 0.005\n",
                        "  Position 650: 0.005\n",
                        "  Position 651: 0.005\n",
                        "  Position 652: 0.005\n",
                        "  Position 653: 0.005\n",
                        "  Position 654: 0.005\n",
                        "  Position 655: 0.005\n",
                        "  Position 656: 0.005\n",
                        "  Position 657: 0.005\n",
                        "  Position 658: 0.005\n",
                        "  Position 659: 0.005\n",
                        "  Position 660: 0.005\n",
                        "  Position 661: 0.005\n",
                        "  Position 662: 0.005\n",
                        "  Position 663: 0.005\n",
                        "  Position 664: 0.005\n",
                        "  Position 665: 0.005\n",
                        "  Position 666: 0.005\n",
                        "  Position 667: 0.005\n",
                        "  Position 668: 0.005\n",
                        "  Position 669: 0.005\n",
                        "  Position 670: 0.005\n",
                        "  Position 671: 0.005\n",
                        "  Position 672: 0.005\n",
                        "  Position 673: 0.005\n",
                        "  Position 674: 0.005\n",
                        "  Position 675: 0.005\n",
                        "  Position 676: 0.005\n",
                        "  Position 677: 0.005\n",
                        "  Position 678: 0.005\n",
                        "  Position 679: 0.005\n",
                        "  Position 680: 0.005\n",
                        "  Position 681: 0.005\n",
                        "  Position 682: 0.005\n",
                        "  Position 683: 0.005\n",
                        "  Position 684: 0.005\n",
                        "  Position 685: 0.005\n",
                        "  Position 686: 0.005\n",
                        "  Position 687: 0.005\n",
                        "  Position 688: 0.005\n",
                        "  Position 689: 0.005\n",
                        "  Position 690: 0.005\n",
                        "  Position 691: 0.005\n",
                        "  Position 692: 0.005\n",
                        "  Position 693: 0.005\n",
                        "  Position 694: 0.005\n",
                        "  Position 695: 0.005\n",
                        "  Position 696: 0.005\n",
                        "  Position 697: 0.005\n",
                        "  Position 698: 0.005\n",
                        "  Position 699: 0.005\n",
                        "  Position 700: 0.005\n",
                        "  Position 701: 0.005\n",
                        "  Position 702: 0.005\n",
                        "  Position 703: 0.005\n",
                        "  Position 704: 0.005\n",
                        "  Position 705: 0.005\n",
                        "  Position 706: 0.005\n",
                        "  Position 707: 0.005\n",
                        "  Position 708: 0.005\n",
                        "  Position 709: 0.005\n",
                        "  Position 710: 0.005\n",
                        "  Position 711: 0.005\n",
                        "  Position 712: 0.005\n",
                        "  Position 713: 0.005\n",
                        "  Position 714: 0.005\n",
                        "  Position 715: 0.005\n",
                        "  Position 716: 0.005\n",
                        "  Position 717: 0.005\n",
                        "  Position 718: 0.005\n",
                        "  Position 719: 0.005\n",
                        "  Position 720: 0.005\n",
                        "  Position 721: 0.005\n",
                        "  Position 722: 0.005\n",
                        "  Position 723: 0.005\n",
                        "  Position 724: 0.005\n",
                        "  Position 725: 0.005\n",
                        "  Position 726: 0.005\n",
                        "  Position 727: 0.005\n",
                        "  Position 728: 0.005\n",
                        "  Position 729: 0.005\n",
                        "  Position 730: 0.005\n",
                        "  Position 731: 0.005\n",
                        "  Position 732: 0.005\n",
                        "  Position 733: 0.005\n",
                        "  Position 734: 0.005\n",
                        "  Position 735: 0.005\n",
                        "  Position 736: 0.005\n",
                        "  Position 737: 0.005\n",
                        "  Position 738: 0.005\n",
                        "  Position 739: 0.005\n",
                        "  Position 740: 0.005\n",
                        "  Position 741: 0.005\n",
                        "  Position 742: 0.005\n",
                        "  Position 743: 0.005\n",
                        "  Position 744: 0.005\n",
                        "  Position 745: 0.005\n",
                        "  Position 746: 0.005\n",