{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dyad Position Predictor\n",
    "Train a neural network to predict dyad positions on a per-position basis from encoded DNA sequences (0-7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from ChromatinFibers import simulate_chromatin_fibers, SequencePlotter  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_reads = 50000\n",
    "n_bp = 10000\n",
    "\n",
    "filename = rf'data/LLM models/dyad_predictor {n_reads}_{n_bp}.pt'\n",
    "plotter=SequencePlotter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñç         | 2349/50000 [1:18:08<36:19:06,  2.74s/it]"
     ]
    }
   ],
   "source": [
    "# Gnerate data if filename does not exist\n",
    "data_filename = filename.replace('.pt', '.npz')\n",
    "if Path(data_filename).exists() is False:\n",
    "    dyad_positions, _, encoded_seq = simulate_chromatin_fibers(n_samples=n_reads, length=n_bp)\n",
    "\n",
    "    np.savez_compressed(\n",
    "        data_filename,\n",
    "        dyad_positions=np.array(dyad_positions, dtype=object),\n",
    "        encoded_seq=np.array(encoded_seq, dtype=object), \n",
    "    )\n",
    "    print(\"Data saved:\", data_filename)  \n",
    "else:\n",
    "    print(\"Data already exists, skipping simulation.\")\n",
    "    data = np.load(data_filename, allow_pickle=True)\n",
    "    dyad_positions = data[\"dyad_positions\"]\n",
    "    encoded_seq = data[\"encoded_seq\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define the Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DyadPredictor(nn.Module):\n",
    "    \"\"\"Per-position dyad predictor using Conv1d and bidirectional context.\"\"\"\n",
    "    def __init__(self, vocab_size=8, embedding_dim=16, hidden_dim=64, num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Embedding layer (map 0-7 to dense vectors)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Conv blocks for local context\n",
    "        self.conv1 = nn.Conv1d(embedding_dim, hidden_dim, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # BiLSTM for global context\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim // 2, num_layers=num_layers, \n",
    "                           bidirectional=True, dropout=dropout if num_layers > 1 else 0, \n",
    "                           batch_first=True)\n",
    "        \n",
    "        # Output head\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\n",
    "        Args:\n",
    "            x: (batch_size, seq_len) - encoded sequence\n",
    "        Returns:\n",
    "            logits: (batch_size, seq_len, 1) - per-position dyad logits\n",
    "        \"\"\"\n",
    "        # Embedding: (batch, seq_len) -> (batch, seq_len, embed_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Conv blocks: (batch, seq_len, embed_dim) -> (batch, embed_dim, seq_len)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        # Back to (batch, seq_len, hidden_dim)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # LSTM: (batch, seq_len, hidden_dim) -> (batch, seq_len, hidden_dim)\n",
    "        x, _ = self.lstm(x)\n",
    "        \n",
    "        # Per-position classification: (batch, seq_len, hidden_dim) -> (batch, seq_len, 1)\n",
    "        logits = self.fc(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DyadDataset(Dataset):\n",
    "    \"\"\"Dataset for dyad position prediction.\"\"\"\n",
    "    def __init__(self, data_list, max_seq_len=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_list: list of tuples (dyad_positions, encoded_sequence)\n",
    "                - dyad_positions: list of integers (positions with dyads)\n",
    "                - encoded_sequence: list/array of ints (0-7)\n",
    "            max_seq_len: optional, pad/truncate sequences to this length\n",
    "        \"\"\"\n",
    "        self.data = data_list\n",
    "        self.max_seq_len = max_seq_len or max(len(seq) for _, seq in data_list)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dyad_pos, encoded_seq = self.data[idx]\n",
    "        seq_len = len(encoded_seq)\n",
    "        \n",
    "        # Create binary label: 1 if dyad at position, 0 otherwise\n",
    "        label = np.zeros(seq_len, dtype=np.float32)\n",
    "        for pos in dyad_pos:\n",
    "            if 0 <= pos < seq_len:\n",
    "                label[pos] = 1.0\n",
    "        \n",
    "        # Convert to tensors\n",
    "        seq_tensor = torch.LongTensor(encoded_seq)\n",
    "        label_tensor = torch.FloatTensor(label)\n",
    "        \n",
    "        # Pad/truncate to max_seq_len\n",
    "        if seq_len < self.max_seq_len:\n",
    "            pad_len = self.max_seq_len - seq_len\n",
    "            seq_tensor = torch.nn.functional.pad(seq_tensor, (0, pad_len), value=0)\n",
    "            label_tensor = torch.nn.functional.pad(label_tensor, (0, pad_len), value=-1)  # -1 for padding\n",
    "        elif seq_len > self.max_seq_len:\n",
    "            seq_tensor = seq_tensor[:self.max_seq_len]\n",
    "            label_tensor = label_tensor[:self.max_seq_len]\n",
    "        \n",
    "        return seq_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Your Data\n",
    "Replace this with your actual data loading logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE: Generate synthetic data\n",
    "# def generate_synthetic_data(n_samples=500, length = 10_000):\n",
    "#     \"\"\"Generate synthetic dyad/sequence pairs for demonstration.\"\"\"\n",
    "#     dyad_positions, _, encoded_seq = simulate_chromatin_fibers(n_samples=n_samples, length=10_000)\n",
    "#     data = [(dyads, seq) for dyads, seq in zip(dyad_positions, encoded_seq)]\n",
    "#     return data\n",
    "\n",
    "\n",
    "# # Generate or load your data here\n",
    "# print(\"Loading data...\")\n",
    "# all_data = generate_synthetic_data(n_samples=n_reads, length=n_bp)  # Replace with your data loading\n",
    "\n",
    "all_data = [(dyads, seq) for dyads, seq in zip(dyad_positions, encoded_seq)]\n",
    "\n",
    "# Split into train/val/test\n",
    "n_train = int(0.7 * len(all_data))\n",
    "n_val = int(0.15 * len(all_data))\n",
    "\n",
    "train_data = all_data[:n_train]\n",
    "val_data = all_data[n_train:n_train+n_val]\n",
    "test_data = all_data[n_train+n_val:]\n",
    "\n",
    "print(f\"Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and dataloaders\n",
    "train_dataset = DyadDataset(train_data)\n",
    "val_dataset = DyadDataset(val_data, max_seq_len=train_dataset.max_seq_len)\n",
    "test_dataset = DyadDataset(test_data, max_seq_len=train_dataset.max_seq_len)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Max sequence length: {train_dataset.max_seq_len}\")\n",
    "print(f\"DataLoaders created: train={len(train_loader)} batches, val={len(val_loader)}, test={len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Initialize Model and Training Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = DyadPredictor(vocab_size=8, embedding_dim=16, hidden_dim=64, num_layers=2, dropout=0.3)\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function: BCEWithLogitsLoss (combines sigmoid + BCE)\n",
    "pos_weight = torch.tensor([10.0]).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight, reduction='none')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "# ReduceLROnPlateau without 'verbose' for compatibility with older PyTorch versions\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a pos_weight from training data (ignore padding where label == -1)\n",
    "\n",
    "def compute_pos_weight_from_dataset(dataset):\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for seq_tensor, label_tensor in dataset:\n",
    "        arr = np.asarray(label_tensor)\n",
    "        mask = arr >= 0  # only count real positions, not padding (-1)\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        pos += int((arr[mask] == 1).sum())\n",
    "        neg += int((arr[mask] == 0).sum())\n",
    "    # Avoid division by zero\n",
    "    if pos == 0:\n",
    "        print(\"Warning: no positive examples found in train_dataset; setting pos=1 to avoid division by zero\")\n",
    "        pos = 1\n",
    "    # pos_weight used by BCEWithLogitsLoss scales the positive class loss: pos_weight = neg/pos\n",
    "    pw = float(neg) / float(pos)\n",
    "    return torch.tensor([pw], dtype=torch.float32)\n",
    "\n",
    "# Compute and clamp pos_weight to a reasonable range to avoid extreme scaling\n",
    "pos_weight = compute_pos_weight_from_dataset(train_dataset).to(device)\n",
    "pos_weight = torch.clamp(pos_weight, min=1.0, max=100.0)\n",
    "print(f\"Computed pos_weight: {pos_weight.item():.4f}\")\n",
    "\n",
    "# Define the weighted criterion (reduction='none' so you can mask padding later)\n",
    "criterion_weighted = nn.BCEWithLogitsLoss(pos_weight=pos_weight, reduction='none')\n",
    "\n",
    "# Expose for REPL visibility\n",
    "criterion_weighted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for seq_batch, label_batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        seq_batch = seq_batch.to(device)  # (batch, seq_len)\n",
    "        label_batch = label_batch.to(device)  # (batch, seq_len)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(seq_batch)  # (batch, seq_len, 1)\n",
    "        logits = logits.squeeze(-1)  # (batch, seq_len)\n",
    "        \n",
    "        # Compute loss (ignore padding positions with label=-1)\n",
    "        loss_per_pos = criterion(logits, label_batch)  # (batch, seq_len)\n",
    "        mask = (label_batch >= 0).float()  # Mask out padding\n",
    "        loss = (loss_per_pos * mask).sum() / mask.sum().clamp(min=1)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for seq_batch, label_batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "            seq_batch = seq_batch.to(device)\n",
    "            label_batch = label_batch.to(device)\n",
    "            \n",
    "            logits = model(seq_batch)\n",
    "            logits = logits.squeeze(-1)\n",
    "            \n",
    "            loss_per_pos = criterion(logits, label_batch)\n",
    "            mask = (label_batch >= 0).float()\n",
    "            loss = (loss_per_pos * mask).sum() / mask.sum().clamp(min=1)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Store for metrics\n",
    "            probs = torch.sigmoid(logits)\n",
    "            all_preds.append(probs.cpu().numpy())\n",
    "            all_labels.append(label_batch.cpu().numpy())\n",
    "    \n",
    "    return total_loss / len(val_loader), all_preds, all_labels\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(epochs):\n",
    "   \n",
    "    train_loss = train_epoch(model, train_loader, criterion_weighted, optimizer, device)   \n",
    "    val_loss, _, _ = validate(model, val_loader, criterion_weighted, device)\n",
    "\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save(model.state_dict(), filename)\n",
    "        print(f\"  Saved best model to {filename}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 3))\n",
    "plt.plot(train_losses, label='Train Loss', marker='o')\n",
    "plt.plot(val_losses, label='Val Loss', marker='s')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.ylim(0, max(max(train_losses), max(val_losses)) * 1.1)\n",
    "plt.xlim(0, 50)\n",
    "plt.tight_layout()\n",
    "plotter.add_caption(f'Training history for {n_reads} reads and {n_bp} bp.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load(filename))\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "# test_loss, test_preds, test_labels = validate(model, test_loader, criterion, device)\n",
    "test_loss, test_preds, test_labels = validate(model, test_loader, criterion_weighted, device)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Compute metrics\n",
    "all_preds_flat = np.concatenate(test_preds).ravel()\n",
    "all_labels_flat = np.concatenate(test_labels).ravel()\n",
    "\n",
    "# Remove padding positions\n",
    "valid_mask = all_labels_flat >= 0\n",
    "all_preds_flat = all_preds_flat[valid_mask]\n",
    "all_labels_flat = all_labels_flat[valid_mask]\n",
    "\n",
    "# Threshold at 0.3 (lower for imbalanced data with weighted loss)\n",
    "predictions_binary = (all_preds_flat >= 0.3).astype(int)\n",
    "\n",
    "\n",
    "# Metrics: try sklearn, fallback to numpy implementations if missing\n",
    "try:\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "    sklearn_available = True\n",
    "except Exception as e:\n",
    "    sklearn_available = False\n",
    "    import warnings\n",
    "    warnings.warn(\"scikit-learn not installed; using numpy fallback for basic metrics. Install with: pip install scikit-learn\")\n",
    "    def accuracy_score(y_true, y_pred):\n",
    "        y_true = np.asarray(y_true)\n",
    "        y_pred = np.asarray(y_pred)\n",
    "        return float((y_true == y_pred).mean())\n",
    "    def precision_score(y_true, y_pred, zero_division=0):\n",
    "        y_true = np.asarray(y_true)\n",
    "        y_pred = np.asarray(y_pred)\n",
    "        tp = int(((y_true == 1) & (y_pred == 1)).sum())\n",
    "        fp = int(((y_true == 0) & (y_pred == 1)).sum())\n",
    "        denom = tp + fp\n",
    "        if denom == 0:\n",
    "            return float(zero_division)\n",
    "        return float(tp / denom)\n",
    "    def recall_score(y_true, y_pred, zero_division=0):\n",
    "        y_true = np.asarray(y_true)\n",
    "        y_pred = np.asarray(y_pred)\n",
    "        tp = int(((y_true == 1) & (y_pred == 1)).sum())\n",
    "        fn = int(((y_true == 1) & (y_pred == 0)).sum())\n",
    "        denom = tp + fn\n",
    "        if denom == 0:\n",
    "            return float(zero_division)\n",
    "        return float(tp / denom)\n",
    "    def f1_score(y_true, y_pred, zero_division=0):\n",
    "        p = precision_score(y_true, y_pred, zero_division)\n",
    "        r = recall_score(y_true, y_pred, zero_division)\n",
    "        if (p + r) == 0:\n",
    "            return float(zero_division)\n",
    "        return 2 * (p * r) / (p + r)\n",
    "    def roc_auc_score(y_true, y_score):\n",
    "        y_true = np.asarray(y_true)\n",
    "        y_score = np.asarray(y_score)\n",
    "        # require both classes present\n",
    "        if len(np.unique(y_true)) < 2:\n",
    "            return float('nan')\n",
    "        # Sort scores descending\n",
    "        desc = np.argsort(-y_score)\n",
    "        y_true_sorted = y_true[desc]\n",
    "        # cumulative true/false positives\n",
    "        tp = np.cumsum(y_true_sorted == 1)\n",
    "        fp = np.cumsum(y_true_sorted == 0)\n",
    "        tp_total = tp[-1]\n",
    "        fp_total = fp[-1]\n",
    "        if tp_total == 0 or fp_total == 0:\n",
    "            return float('nan')\n",
    "        tpr = np.concatenate([[0.0], tp / tp_total])\n",
    "        fpr = np.concatenate([[0.0], fp / fp_total])\n",
    "        return float(np.trapz(tpr, fpr))\n",
    "\n",
    "# Calculate metrics\n",
    "acc = accuracy_score(all_labels_flat, predictions_binary)\n",
    "prec = precision_score(all_labels_flat, predictions_binary, zero_division=0)\n",
    "rec = recall_score(all_labels_flat, predictions_binary, zero_division=0)\n",
    "f1 = f1_score(all_labels_flat, predictions_binary, zero_division=0)\n",
    "auc = roc_auc_score(all_labels_flat, all_preds_flat)\n",
    "\n",
    "print(f\"\\nTest Metrics:\")\n",
    "print(f\"  Accuracy:  {acc:.4f}\")\n",
    "print(f\"  Precision: {prec:.4f}\")\n",
    "print(f\"  Recall:    {rec:.4f}\")\n",
    "print(f\"  F1-Score:  {f1:.4f}\")\n",
    "print(f\"  ROC-AUC:   {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Save Model for Later Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and configuration\n",
    "model_config = {\n",
    "    '_vocab_size': 8,\n",
    "    '_embedding_dim': 16,\n",
    "    '_hidden_dim': 64,\n",
    "    '_num_layers': 2,\n",
    "    '_dropout': 0.3,\n",
    "    'max_seq_len': train_dataset.max_seq_len,\n",
    "    'n_reads': n_reads,\n",
    "    'n_bp': n_bp\n",
    "}\n",
    "\n",
    "# Save config\n",
    "with open(Path(filename).with_suffix('.json'), 'w') as f:\n",
    "    json.dump(model_config, f, indent=2)\n",
    "\n",
    "# Model is already saved as 'best_dyad_predictor.pt'\n",
    "print(\"Model and config saved!\")\n",
    "print(f\"Config: {model_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Load Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_dyads(model, encoded_sequence, threshold=0.2, device='cpu'):\n",
    "    \"\"\"\n",
    "    Predict dyad positions for a single sequence.\n",
    "    \n",
    "    Args:\n",
    "        model: trained DyadPredictor\n",
    "        encoded_sequence: list/array of integers (0-7)\n",
    "        threshold: probability threshold for positive class (default 0.5)\n",
    "        device: torch device\n",
    "    \n",
    "    Returns:\n",
    "        dyad_positions: list of predicted dyad positions\n",
    "        probabilities: array of per-position probabilities\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        seq_tensor = torch.LongTensor(encoded_sequence).unsqueeze(0).to(device)\n",
    "        logits = model(seq_tensor)\n",
    "        probs = torch.sigmoid(logits).squeeze().cpu().numpy()\n",
    "    \n",
    "    dyad_positions = np.where(probs >= threshold)[0].tolist()\n",
    "    return dyad_positions, probs\n",
    "\n",
    "\n",
    "# Load config\n",
    "with open(Path(filename).with_suffix('.json'), 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "\n",
    "# Create model\n",
    "loaded_model = DyadPredictor(**{k[1:]: v for k, v in config.items() if k[0] == '_'})\n",
    "loaded_model.load_state_dict(torch.load(filename))\n",
    "loaded_model = loaded_model.to(device)\n",
    "loaded_model.eval()\n",
    "\n",
    "# Use for predictions\n",
    "new_seq = [1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4]\n",
    "\n",
    "dyad_positions, _, encoded_seq = simulate_chromatin_fibers(n_samples=1, length=n_bp)\n",
    "\n",
    "new_data = [(dyad_positions[0], encoded_seq[0])]\n",
    "new_seq = new_data[0][1]\n",
    "\n",
    "dyads, probs = predict_dyads(loaded_model, new_seq, device=device)\n",
    "\n",
    "index = np.arange(len(new_seq))\n",
    "nucs = np.zeros_like(index)\n",
    "nucs[new_data[0][0]] = 1.0\n",
    "\n",
    "for i in new_data[0][0]:\n",
    "    nucs[i-65:i+65] = -1   # highlight nucleosome region\n",
    "\n",
    "n_plots = 10\n",
    "for i in range(n_plots):\n",
    "    plt.figure(figsize=(12, 1))\n",
    "    plt.xlabel(\"i (bp)\")\n",
    "\n",
    "    plt.vlines(new_data[0][0], ymin=-1, ymax=2, color='black',linestyles= 'dotted', alpha=1)\n",
    "    plt.fill_between(index, probs, label='Predicted Dyad Probability', color='orange', alpha=0.5)\n",
    "    plt.fill_between(index, nucs, color='blue', alpha=0.5, label='True Dyad Positions')\n",
    "\n",
    "\n",
    "    methylations = np.zeros_like(index) -1\n",
    "    methylations[index[new_seq>4]] = 1\n",
    "    plt.plot(index, methylations, 'o', label='methylations', color='green', alpha=0.5, fillstyle='full', markersize=3)\n",
    "\n",
    "    plt.xlim(i*len(index)//n_plots, (i+1)*len(index)//n_plots)\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.ylim(-0.1, 1.1)\n",
    "    \n",
    "    # plt.tight_layout\n",
    "    plt.show()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
