{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Learning Methylated Sequence to Dyad Relationship\n",
                "Train a neural network to discover the relationship between 6-letter methylated sequences and dyad positions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from torch.utils.data import DataLoader, TensorDataset\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define a simple model\n",
                "class DyadPredictor(nn.Module):\n",
                "    def __init__(self, seq_len=6, hidden_dim=32):\n",
                "        super().__init__()\n",
                "        # Embed 6-letter alphabet to vectors\n",
                "        self.embed = nn.Embedding(6, 8)\n",
                "        \n",
                "        # Process sequence with Conv1d\n",
                "        self.conv = nn.Conv1d(8, hidden_dim, kernel_size=3, padding=1)\n",
                "        self.fc = nn.Sequential(\n",
                "            nn.Linear(hidden_dim * seq_len, 32),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(32, 1)  # Predict dyad position (0-1 normalized)\n",
                "        )\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = self.embed(x)  # (batch, seq_len) -> (batch, seq_len, 8)\n",
                "        x = x.transpose(1, 2)  # (batch, 8, seq_len)\n",
                "        x = self.conv(x)  # (batch, hidden_dim, seq_len)\n",
                "        x = x.flatten(1)  # (batch, hidden_dim * seq_len)\n",
                "        x = self.fc(x)\n",
                "        return torch.sigmoid(x)  # Normalize to [0, 1]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare training data\n",
                "# Example: map 6-letter sequence to dyad position\n",
                "# Replace with your actual data!\n",
                "\n",
                "def generate_synthetic_data(n_samples=1000, seq_len=6):\n",
                "    \"\"\"Generate synthetic methylated sequences and corresponding dyads.\"\"\"\n",
                "    sequences = np.random.randint(0, 6, (n_samples, seq_len))\n",
                "    \n",
                "    # Simple rule: dyad position depends on sequence pattern\n",
                "    # Example: sum of letters modulo 1\n",
                "    dyads = (sequences.sum(axis=1) / (6 * 6)) % 1.0\n",
                "    \n",
                "    return sequences, dyads\n",
                "\n",
                "X, y = generate_synthetic_data()\n",
                "X_tensor = torch.LongTensor(X)\n",
                "y_tensor = torch.FloatTensor(y).reshape(-1, 1)\n",
                "\n",
                "dataset = TensorDataset(X_tensor, y_tensor)\n",
                "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train the model\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "model = DyadPredictor().to(device)\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
                "criterion = nn.MSELoss()\n",
                "\n",
                "epochs = 50\n",
                "losses = []\n",
                "\n",
                "for epoch in range(epochs):\n",
                "    epoch_loss = 0\n",
                "    for batch_X, batch_y in train_loader:\n",
                "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        pred = model(batch_X)\n",
                "        loss = criterion(pred, batch_y)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        epoch_loss += loss.item()\n",
                "    \n",
                "    avg_loss = epoch_loss / len(train_loader)\n",
                "    losses.append(avg_loss)\n",
                "    if (epoch + 1) % 10 == 0:\n",
                "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
                "\n",
                "plt.figure(figsize=(8, 4))\n",
                "plt.plot(losses)\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Loss')\n",
                "plt.title('Training Loss')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test the model\n",
                "model.eval()\n",
                "with torch.no_grad():\n",
                "    test_seq = torch.LongTensor([[0, 1, 2, 3, 4, 5]]).to(device)\n",
                "    pred = model(test_seq)\n",
                "    print(f\"Test sequence: {test_seq.cpu().numpy()}\")\n",
                "    print(f\"Predicted dyad position (normalized): {pred.cpu().item():.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## To use with your data:\n",
                "1. Provide your `methylated_sequence` (6 letters, encoded 0-5)\n",
                "2. Provide corresponding `dyads` positions (normalized 0-1 or as indices)\n",
                "3. Replace `generate_synthetic_data()` with your actual data\n",
                "4. Adjust model architecture if needed"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}